"""Ğ¡ĞºÑ€ĞµĞ¹Ğ¿ĞµÑ€ Ğ´Ğ»Ñ Ğ¯Ğ½Ğ´ĞµĞºÑ Ğ›Ğ°Ğ²ĞºĞ°."""

import asyncio
import re
from typing import List, Dict, Any, Optional
from urllib.parse import urljoin, urlparse
from decimal import Decimal
from playwright.async_api import Page
from loguru import logger

from .base import BaseScraper


class LavkaScraper(BaseScraper):
    """Ğ¡ĞºÑ€ĞµĞ¹Ğ¿ĞµÑ€ Ğ´Ğ»Ñ ÑĞ°Ğ¹Ñ‚Ğ° Ğ¯Ğ½Ğ´ĞµĞºÑ Ğ›Ğ°Ğ²ĞºĞ°."""
    
    @property
    def shop_name(self) -> str:
        return "lavka"
    
    @property
    def base_url(self) -> str:
        return "https://lavka.yandex.ru"
    
    async def _set_location(self) -> None:
        """Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° Ğ¸ Ğ°Ğ´Ñ€ĞµÑĞ° Ğ² Ğ¯Ğ½Ğ´ĞµĞºÑ Ğ›Ğ°Ğ²ĞºĞ°."""
        page = await self.context.new_page()
        
        try:
            logger.info(f"Setting location to {self.config.city}, {self.config.address}")
            
            # ĞŸĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¸Ğ¼ Ğ½Ğ° Ğ³Ğ»Ğ°Ğ²Ğ½ÑƒÑ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ
            await page.goto(self.base_url, wait_until="networkidle")
            await asyncio.sleep(3)
            
            # Ğ˜Ñ‰ĞµĞ¼ Ğ¼Ğ¾Ğ´Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾ĞºĞ½Ğ¾ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ°Ğ´Ñ€ĞµÑĞ°
            address_modal_selectors = [
                '[data-testid="address-modal"]',
                '.address-modal',
                '.location-modal',
                '[class*="modal"]',
                '[role="dialog"]'
            ]
            
            modal_found = False
            for selector in address_modal_selectors:
                try:
                    modal = await page.wait_for_selector(selector, timeout=5000)
                    if modal:
                        modal_found = True
                        break
                except:
                    continue
            
            if not modal_found:
                # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ²Ñ‹Ğ±Ğ¾Ñ€Ğ° Ğ°Ğ´Ñ€ĞµÑĞ°
                address_button_selectors = [
                    '[data-testid="address-button"]',
                    '.address-button',
                    'button[class*="address"]',
                    'button:has-text("Ğ’Ñ‹Ğ±ĞµÑ€Ğ¸Ñ‚Ğµ Ğ°Ğ´Ñ€ĞµÑ")',
                    'button:has-text("ĞĞ´Ñ€ĞµÑ Ğ´Ğ¾ÑÑ‚Ğ°Ğ²ĞºĞ¸")'
                ]
                
                for selector in address_button_selectors:
                    try:
                        button = await page.wait_for_selector(selector, timeout=3000)
                        if button:
                            await button.click()
                            await asyncio.sleep(2)
                            break
                    except:
                        continue
            
            # Ğ˜Ñ‰ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ²Ğ²Ğ¾Ğ´Ğ° Ğ°Ğ´Ñ€ĞµÑĞ°
            address_input_selectors = [
                'input[placeholder*="Ğ°Ğ´Ñ€ĞµÑ"]',
                'input[placeholder*="ĞĞ´Ñ€ĞµÑ"]',
                'input[name="address"]',
                'input[data-testid="address-input"]',
                '.address-input input',
                'input[type="text"]'
            ]
            
            address_input = None
            for selector in address_input_selectors:
                try:
                    address_input = await page.wait_for_selector(selector, timeout=5000)
                    if address_input:
                        break
                except:
                    continue
            
            if address_input:
                # ĞÑ‡Ğ¸Ñ‰Ğ°ĞµĞ¼ Ğ¿Ğ¾Ğ»Ğµ Ğ¸ Ğ²Ğ²Ğ¾Ğ´Ğ¸Ğ¼ Ğ°Ğ´Ñ€ĞµÑ
                await address_input.click()
                await address_input.fill("")
                full_address = f"{self.config.city}, {self.config.address}"
                await address_input.type(full_address, delay=100)
                await asyncio.sleep(3)
                
                # Ğ–Ğ´ĞµĞ¼ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·Ğ¾Ğº Ğ¸ ĞºĞ»Ğ¸ĞºĞ°ĞµĞ¼ Ğ½Ğ° Ğ¿ĞµÑ€Ğ²ÑƒÑ
                suggestion_selectors = [
                    '[data-testid="address-suggestion"]',
                    '.address-suggestion',
                    '.suggestion',
                    '.dropdown-item',
                    '.autocomplete-item',
                    '[role="option"]'
                ]
                
                suggestion_clicked = False
                for selector in suggestion_selectors:
                    try:
                        suggestions = await page.wait_for_selector(selector, timeout=5000)
                        if suggestions:
                            await suggestions.click()
                            suggestion_clicked = True
                            break
                    except:
                        continue
                
                if not suggestion_clicked:
                    # Ğ•ÑĞ»Ğ¸ Ğ¿Ğ¾Ğ´ÑĞºĞ°Ğ·ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ Ğ½Ğ°Ğ¶Ğ¸Ğ¼Ğ°ĞµĞ¼ Enter
                    await address_input.press('Enter')
                
                await asyncio.sleep(2)
                
                # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºÑƒ Ğ¿Ğ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ¶Ğ´ĞµĞ½Ğ¸Ñ
                confirm_selectors = [
                    'button:has-text("ĞŸĞ¾Ğ´Ñ‚Ğ²ĞµÑ€Ğ´Ğ¸Ñ‚ÑŒ")',
                    'button:has-text("Ğ’Ñ‹Ğ±Ñ€Ğ°Ñ‚ÑŒ")',
                    'button:has-text("Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾")',
                    'button:has-text("Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ")',
                    '[data-testid="confirm-button"]',
                    '.confirm-button'
                ]
                
                for selector in confirm_selectors:
                    try:
                        confirm_btn = await page.wait_for_selector(selector, timeout=3000)
                        if confirm_btn:
                            await confirm_btn.click()
                            break
                    except:
                        continue
            
            await asyncio.sleep(5)
            logger.info("Location set successfully")
            
        except Exception as e:
            logger.warning(f"Could not set location automatically: {e}")
        finally:
            await page.close()
    
    async def _scrape_items(self) -> List[Dict[str, Any]]:
        """ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ÑĞºĞ° Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞµĞ´Ñ‹."""
        all_items = []
        
        # ĞŸÑ€ÑĞ¼Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¾Ğ¹ ĞµĞ´Ñ‹ Ğ¯Ğ½Ğ´ĞµĞºÑ Ğ›Ğ°Ğ²ĞºĞ°
        category_urls = [
            "https://lavka.yandex.ru/category/gotovaya_eda",
            "https://lavka.yandex.ru/category/hot_streetfood", 
            "https://lavka.yandex.ru/category/gotovaya_eda/ostroe-1"
        ]
        
        for category_url in category_urls:
            try:
                category_name = category_url.split('/')[-1]
                logger.info(f"Scraping category URL: {category_url}")
                category_items = await self._scrape_category_url(category_url, category_name)
                all_items.extend(category_items)
                
                # Ğ—Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼Ğ¸
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"Failed to scrape category {category_url}: {e}")
        
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ‚Ñ‹ Ğ¿Ğ¾ URL
        seen_urls = set()
        unique_items = []
        for item in all_items:
            if item.get('url') not in seen_urls:
                seen_urls.add(item.get('url'))
                unique_items.append(item)
        
        logger.info(f"Found {len(unique_items)} unique items from {len(all_items)} total")
        return unique_items
    
    async def _scrape_category_url(self, category_url: str, category_name: str) -> List[Dict[str, Any]]:
        """Ğ¡ĞºÑ€ĞµĞ¹Ğ¿Ğ¸Ğ½Ğ³ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ¿Ğ¾ Ğ¿Ñ€ÑĞ¼Ğ¾Ğ¼Ñƒ URL ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸."""
        page = await self.context.new_page()
        items = []
        
        try:
            logger.info(f"ğŸŒ Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="domcontentloaded", timeout=60000)
            logger.info(f"âœ… Page loaded successfully")
            
            # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            title = await page.title()
            logger.info(f"ğŸ“„ Page title: {title}")
            
            await asyncio.sleep(5)
            logger.info(f"â±ï¸ Waited 5 seconds for page to settle")
            
            # ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²
            await self._scroll_to_load_all(page)
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹ Ñ Ñ€Ğ°Ğ±Ğ¾Ñ‡Ğ¸Ğ¼Ğ¸ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ›Ğ°Ğ²ĞºĞ°
            product_selectors = [
                '[class*="Product"]',
                '[class*="Card"]',
                '[class*="product"]',
                'a[href*="/product/"]',
                'a[href*="/goods/"]',
                '[data-testid="product-card"]',
                '[data-testid="product"]',
                '.product-card',
                '.product',
                '.catalog-item'
            ]
            
            products = []
            logger.info(f"ğŸ” Testing product selectors for Lavka...")
            
            for selector in product_selectors:
                try:
                    test_products = await page.query_selector_all(selector)
                    logger.info(f"   {selector}: {len(test_products)} elements")
                    if test_products and len(test_products) > len(products):
                        products = test_products
                        logger.info(f"âœ… Best selector so far: {selector} with {len(products)} products")
                except Exception as e:
                    logger.warning(f"   {selector}: ERROR - {e}")
            
            if not products:
                logger.warning(f"âŒ No products found with standard selectors")
                
                # Ğ”Ğ¸Ğ°Ğ³Ğ½Ğ¾ÑÑ‚Ğ¸ĞºĞ°: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
                page_content = await page.content()
                logger.info(f"ğŸ“„ Page content length: {len(page_content)} characters")
                
                if 'Ñ‚Ğ¾Ğ²Ğ°Ñ€' in page_content.lower():
                    logger.info(f"âœ… Page contains word 'Ñ‚Ğ¾Ğ²Ğ°Ñ€'")
                else:
                    logger.warning(f"âŒ Page does not contain word 'Ñ‚Ğ¾Ğ²Ğ°Ñ€'")
                
                if 'product' in page_content.lower():
                    logger.info(f"âœ… Page contains word 'product'")
                else:
                    logger.warning(f"âŒ Page does not contain word 'product'")
                
                # Fallback: Ğ¸Ñ‰ĞµĞ¼ Ğ»ÑĞ±Ñ‹Ğµ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ½Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹
                logger.info(f"ğŸ” Looking for any product links...")
                link_selectors = ['a[href*="/product/"]', 'a[href*="/goods/"]', 'a[href*="/item/"]']
                
                for link_selector in link_selectors:
                    try:
                        links = await page.query_selector_all(link_selector)
                        logger.info(f"   {link_selector}: {len(links)} links")
                        if links:
                            products = links[:20]
                            break
                    except Exception as e:
                        logger.warning(f"   {link_selector}: ERROR - {e}")
                
                if not products:
                    logger.error(f"âŒ No product links found at all!")
                    return items
            
            logger.info(f"Processing {len(products)} products from category {category_name}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category_name)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category_url}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _scrape_category(self, category: str) -> List[Dict[str, Any]]:
        """Ğ¡ĞºÑ€ĞµĞ¹Ğ¿Ğ¸Ğ½Ğ³ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ² Ğ¸Ğ· Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸."""
        page = await self.context.new_page()
        items = []
        
        try:
            # Ğ¤Ğ¾Ñ€Ğ¼Ğ¸Ñ€ÑƒĞµĞ¼ URL ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸
            category_url = f"{self.base_url}/catalog/{category}"
            logger.debug(f"Loading category URL: {category_url}")
            
            await page.goto(category_url, wait_until="networkidle")
            await asyncio.sleep(3)
            
            # ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ğ²ÑĞµÑ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ¾Ğ²
            await self._scroll_to_load_all(page)
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ñ‹
            product_selectors = [
                '[data-testid="product-card"]',
                '[data-testid="product"]',
                '.product-card',
                '.product',
                '[class*="ProductCard"]',
                '[class*="product"]'
            ]
            
            products = []
            for selector in product_selectors:
                try:
                    products = await page.query_selector_all(selector)
                    if products:
                        logger.debug(f"Found {len(products)} products with selector: {selector}")
                        break
                except:
                    continue
            
            if not products:
                logger.warning(f"No products found in category {category}")
                return items
            
            logger.info(f"Processing {len(products)} products from category {category}")
            
            for product in products:
                try:
                    item_data = await self._extract_product_data(product, page, category)
                    if item_data:
                        items.append(item_data)
                except Exception as e:
                    logger.warning(f"Failed to extract product data: {e}")
            
        except Exception as e:
            logger.error(f"Failed to scrape category {category}: {e}")
        finally:
            await page.close()
        
        return items
    
    async def _extract_product_data(self, product_element, page: Page, category: str) -> Optional[Dict[str, Any]]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ¸Ğ· ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸."""
        try:
            # ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° - Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ›Ğ°Ğ²ĞºĞ°
            name_selectors = [
                '[data-testid="product-name"]',
                '[data-testid="product-title"]',
                '.product-name',
                '.product-title',
                '[class*="title"]',
                '[class*="Title"]',
                '[class*="name"]',
                '[class*="Name"]',
                'h1', 'h2', 'h3', 'h4', 'h5',
                'a',
                'span'
            ]
            
            name = None
            for selector in name_selectors:
                try:
                    name_element = await product_element.query_selector(selector)
                    if name_element:
                        name = await name_element.inner_text()
                        if name and name.strip():
                            break
                except:
                    continue
            
            if not name:
                return None
            
            # Ğ¦ĞµĞ½Ğ° - Ñ€Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¸ÑĞº Ğ´Ğ»Ñ Ğ›Ğ°Ğ²ĞºĞ°
            price_selectors = [
                '[data-testid="product-price"]',
                '[data-testid="price"]',
                '.product-price',
                '.price',
                '[class*="Price"]',
                '[class*="price"]',
                '[class*="cost"]',
                '[class*="Cost"]',
                'span:contains("â‚½")',
                'div:contains("â‚½")'
            ]
            
            price = None
            for selector in price_selectors:
                try:
                    price_element = await product_element.query_selector(selector)
                    if price_element:
                        price_text = await price_element.inner_text()
                        price = self._extract_price(price_text)
                        if price:
                            break
                except:
                    continue
            
            if not price:
                return None
            
            # Ğ¡ÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Ñ‚Ğ¾Ğ²Ğ°Ñ€
            link_selectors = [
                'a[href]',
                '[data-testid="product-link"]'
            ]
            
            url = None
            for selector in link_selectors:
                try:
                    link_element = await product_element.query_selector(selector)
                    if link_element:
                        href = await link_element.get_attribute('href')
                        if href:
                            url = urljoin(self.base_url, href)
                            break
                except:
                    continue
            
            # Ğ•ÑĞ»Ğ¸ ÑÑÑ‹Ğ»ĞºĞ° Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ°, Ğ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ½ÑƒĞ¶Ğ½Ğ¾ ĞºĞ»Ğ¸ĞºĞ½ÑƒÑ‚ÑŒ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºÑƒ
            if not url:
                try:
                    # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğ¹ URL
                    current_url = page.url
                    
                    # ĞšĞ»Ğ¸ĞºĞ°ĞµĞ¼ Ğ½Ğ° ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºÑƒ
                    await product_element.click()
                    await asyncio.sleep(1)
                    
                    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼, Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ğ»ÑÑ Ğ»Ğ¸ URL
                    new_url = page.url
                    if new_url != current_url:
                        url = new_url
                        # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ÑÑ Ğ½Ğ°Ğ·Ğ°Ğ´
                        await page.go_back()
                        await asyncio.sleep(1)
                except:
                    pass
            
            if not url:
                return None
            
            # Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ
            image_selectors = [
                'img[src]',
                '[data-testid="product-image"] img',
                '.product-image img'
            ]
            
            photo_url = None
            for selector in image_selectors:
                try:
                    img_element = await product_element.query_selector(selector)
                    if img_element:
                        src = await img_element.get_attribute('src')
                        if src and 'data:' not in src:  # Ğ˜ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ base64 Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
                            if src.startswith('//'):
                                photo_url = f"https:{src}"
                            elif src.startswith('/'):
                                photo_url = urljoin(self.base_url, src)
                            else:
                                photo_url = src
                            break
                except:
                    continue
            
            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ²ĞµÑ Ğ¸Ğ· Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ
            portion_g = self._extract_weight_from_text(name)
            if not portion_g:
                # Ğ˜Ñ‰ĞµĞ¼ Ğ²ĞµÑ Ğ² Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸
                weight_text = await self._extract_text_from_element(product_element, [
                    '[class*="weight"]', '[class*="Weight"]', 
                    '[class*="gram"]', '[class*="Gram"]',
                    '.weight', '.portion'
                ])
                if weight_text:
                    portion_g = self._extract_weight_from_text(weight_text)
            
            # ĞŸÑ‹Ñ‚Ğ°ĞµĞ¼ÑÑ Ğ¸Ğ·Ğ²Ğ»ĞµÑ‡ÑŒ Ğ´Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ· Ğ²Ğ¸Ğ´Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°
            all_text = await product_element.inner_text() if product_element else ""
            
            # Ğ˜Ñ‰ĞµĞ¼ Ñ‚ĞµĞ³Ğ¸/Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ
            tags = self._extract_tags_from_text(all_text)
            
            # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ ID Ğ¸Ğ· URL
            native_id = self._extract_id_from_url(url)
            
            return {
                'native_id': native_id,
                'name': name.strip(),
                'category': category,
                'price': price,
                'url': url,
                'photo_url': photo_url,
                'portion_g': portion_g,
                'tags': tags,
                'composition': None,
                'kcal_100g': None,
                'protein_100g': None,
                'fat_100g': None,
                'carb_100g': None
            }
            
        except Exception as e:
            logger.warning(f"Failed to extract product data: {e}")
            return None
    
    async def _enrich_item_details(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """ĞĞ±Ğ¾Ğ³Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ¿ĞµÑ€ĞµÑ…Ğ¾Ğ´Ğ¾Ğ¼ Ğ² ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºÑƒ."""
        page = await self.context.new_page()
        
        try:
            logger.debug(f"Enriching item: {item_data.get('name')}")
            
            await page.goto(item_data['url'], wait_until="networkidle")
            await asyncio.sleep(3)
            
            # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            
            # Ğ¡Ğ¾ÑÑ‚Ğ°Ğ²/Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚Ñ‹
            composition = await self._extract_composition(page)
            
            # ĞŸĞ¸Ñ‰ĞµĞ²Ğ°Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ
            nutrition_data = await self._extract_nutrition_info(page)
            
            # Ğ’ĞµÑ Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸
            portion_g = await self._extract_portion_weight(page)
            
            # Ğ¢ĞµĞ³Ğ¸/Ğ¾ÑĞ¾Ğ±ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸
            tags = await self._extract_tags(page)
            
            # Ğ”Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ
            brand = await self._extract_brand(page)
            barcode = await self._extract_barcode(page)
            shelf_life = await self._extract_shelf_life(page)
            
            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ
            item_data.update({
                'composition': composition,
                'portion_g': portion_g,
                'tags': tags,
                'brand': brand,
                'barcode': barcode,
                'shelf_life': shelf_life,
                **nutrition_data
            })
            
        except Exception as e:
            logger.warning(f"Failed to enrich item {item_data.get('url')}: {e}")
        finally:
            await page.close()
        
        return item_data
    
    async def _extract_composition(self, page: Page) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ°."""
        composition_selectors = [
            '[data-testid="composition"]',
            '[data-testid="ingredients"]',
            '.composition',
            '.ingredients',
            '*:has-text("Ğ¡Ğ¾ÑÑ‚Ğ°Ğ²")',
            '*:has-text("Ğ˜Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚Ñ‹")'
        ]
        
        for selector in composition_selectors:
            try:
                if ':has-text(' in selector:
                    # Ğ”Ğ»Ñ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ² Ñ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ğ¸Ñ‰ĞµĞ¼ Ñ€Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if 'ÑĞ¾ÑÑ‚Ğ°Ğ²' in text.lower() or 'Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚' in text.lower():
                                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ‡Ğ°ÑÑ‚ÑŒ Ñ ÑĞ¾ÑÑ‚Ğ°Ğ²Ğ¾Ğ¼
                                lines = text.split('\n')
                                for i, line in enumerate(lines):
                                    if 'ÑĞ¾ÑÑ‚Ğ°Ğ²' in line.lower() or 'Ğ¸Ğ½Ğ³Ñ€ĞµĞ´Ğ¸ĞµĞ½Ñ‚' in line.lower():
                                        composition_lines = lines[i+1:]
                                        composition = '\n'.join(composition_lines).strip()
                                        if composition:
                                            return composition
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text and text.strip():
                            return text.strip()
            except:
                continue
        
        return None
    
    async def _extract_nutrition_info(self, page: Page) -> Dict[str, Any]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸ Ğ¾ Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚Ğ¸."""
        nutrition_data = {}
        
        # Ğ˜Ñ‰ĞµĞ¼ Ğ±Ğ»Ğ¾Ğº Ñ Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ¾Ğ¹ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒÑ
        nutrition_selectors = [
            '[data-testid="nutrition"]',
            '[data-testid="nutritional-value"]',
            '.nutrition',
            '.nutritional-value',
            '.energy-value',
            '*:has-text("ĞŸĞ¸Ñ‰ĞµĞ²Ğ°Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ")',
            '*:has-text("Ğ­Ğ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ñ†ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ")',
            '*:has-text("Ğ½Ğ° 100")'
        ]
        
        nutrition_text = ""
        
        for selector in nutrition_selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if any(keyword in text.lower() for keyword in ['Ğ¿Ğ¸Ñ‰ĞµĞ²Ğ°Ñ', 'ÑĞ½ĞµÑ€Ğ³ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ', 'ĞºĞ°Ğ»Ğ¾Ñ€', 'Ğ±ĞµĞ»Ğº', 'Ğ¶Ğ¸Ñ€', 'ÑƒĞ³Ğ»ĞµĞ²Ğ¾Ğ´']):
                                nutrition_text += text + "\n"
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text:
                            nutrition_text += text + "\n"
            except:
                continue
        
        if not nutrition_text:
            # Ğ˜Ñ‰ĞµĞ¼ Ğ² Ğ¾Ğ±Ñ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
            try:
                page_content = await page.content()
                nutrition_text = page_content
            except:
                pass
        
        if nutrition_text:
            # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ½ÑƒÑ‚Ñ€Ğ¸ĞµĞ½Ñ‚Ñ‹
            
            # ĞšĞ°Ğ»Ğ¾Ñ€Ğ¸Ğ¸
            kcal_patterns = [
                r'(\d+(?:[.,]\d+)?)\s*(?:ĞºĞºĞ°Ğ»|kcal)',
                r'ĞºĞ°Ğ»Ğ¾Ñ€.*?(\d+(?:[.,]\d+)?)',
                r'ÑĞ½ĞµÑ€Ğ³ĞµÑ‚.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in kcal_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['kcal_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # Ğ‘ĞµĞ»ĞºĞ¸
            protein_patterns = [
                r'Ğ±ĞµĞ»Ğº[Ğ¸|Ğ°].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*Ğ³.*Ğ±ĞµĞ»Ğº',
                r'protein.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in protein_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['protein_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # Ğ–Ğ¸Ñ€Ñ‹
            fat_patterns = [
                r'Ğ¶Ğ¸Ñ€[Ñ‹|Ğ°].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*Ğ³.*Ğ¶Ğ¸Ñ€',
                r'fat.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in fat_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['fat_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
            
            # Ğ£Ğ³Ğ»ĞµĞ²Ğ¾Ğ´Ñ‹
            carb_patterns = [
                r'ÑƒĞ³Ğ»ĞµĞ²Ğ¾Ğ´[Ñ‹|Ğ°].*?(\d+(?:[.,]\d+)?)',
                r'(\d+(?:[.,]\d+)?)\s*Ğ³.*ÑƒĞ³Ğ»ĞµĞ²Ğ¾Ğ´',
                r'carb.*?(\d+(?:[.,]\d+)?)'
            ]
            
            for pattern in carb_patterns:
                match = re.search(pattern, nutrition_text, re.IGNORECASE)
                if match:
                    nutrition_data['carb_100g'] = Decimal(match.group(1).replace(',', '.'))
                    break
        
        return nutrition_data
    
    async def _extract_portion_weight(self, page: Page) -> Optional[Decimal]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ° Ğ¿Ğ¾Ñ€Ñ†Ğ¸Ğ¸."""
        # Ğ˜Ñ‰ĞµĞ¼ Ğ²ĞµÑ Ğ² Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¼ĞµÑÑ‚Ğ°Ñ…
        weight_selectors = [
            '[data-testid="weight"]',
            '.weight',
            '.portion-weight',
            '*:has-text("Ğ’ĞµÑ")',
            '*:has-text("ĞœĞ°ÑÑĞ°")',
            '*:has-text(" Ğ³")'
        ]
        
        for selector in weight_selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        text = await element.inner_text()
                        weight_match = re.search(r'(\d+(?:[.,]\d+)?)\s*Ğ³', text)
                        if weight_match:
                            return Decimal(weight_match.group(1).replace(',', '.'))
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        weight_match = re.search(r'(\d+(?:[.,]\d+)?)\s*Ğ³', text)
                        if weight_match:
                            return Decimal(weight_match.group(1).replace(',', '.'))
            except:
                continue
        
        # Ğ˜Ñ‰ĞµĞ¼ Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
        try:
            title = await page.title()
            weight_match = re.search(r'(\d+)\s*Ğ³', title)
            if weight_match:
                return Decimal(weight_match.group(1))
        except:
            pass
        
        return None
    
    async def _extract_tags(self, page: Page) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ³Ğ¾Ğ² Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ°."""
        tags = []
        
        # Ğ˜Ñ‰ĞµĞ¼ Ñ‚ĞµĞ³Ğ¸ Ğ² ÑĞ¿ĞµÑ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ…
        tag_selectors = [
            '[data-testid="tags"]',
            '[data-testid="labels"]',
            '.tags',
            '.labels',
            '.badges'
        ]
        
        for selector in tag_selectors:
            try:
                elements = await page.query_selector_all(f'{selector} *')
                for element in elements:
                    text = await element.inner_text()
                    if text and len(text.strip()) > 1:
                        tags.append(text.strip().lower())
            except:
                continue
        
        # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ² ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğ¼
        try:
            page_content = await page.content()
            keywords = ['Ğ¾ÑÑ‚Ñ€Ğ¾Ğµ', 'Ğ¾ÑÑ‚Ñ€Ñ‹Ğ¹', 'Ğ²ĞµĞ³ĞµÑ‚Ğ°Ñ€Ğ¸Ğ°Ğ½ÑĞºĞ¸Ğ¹', 'Ğ²ĞµĞ³Ğ°Ğ½', 'Ğ¿Ğ¿', 'Ğ´Ğ¸ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 
                       'Ğ±ĞµĞ· Ğ³Ğ»ÑÑ‚ĞµĞ½Ğ°', 'Ğ±ĞµĞ· Ğ»Ğ°ĞºÑ‚Ğ¾Ğ·Ñ‹', 'Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 'ÑĞºĞ¾', 'bio']
            for keyword in keywords:
                if keyword in page_content.lower():
                    tags.append(keyword)
        except:
            pass
        
        return list(set(tags))
    
    async def _extract_brand(self, page: Page) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ±Ñ€ĞµĞ½Ğ´Ğ°."""
        brand_selectors = [
            '[data-testid="brand"]',
            '.brand',
            '.manufacturer',
            '*:has-text("ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ")',
            '*:has-text("Ğ‘Ñ€ĞµĞ½Ğ´")'
        ]
        
        return await self._extract_text_by_selectors(page, brand_selectors)
    
    async def _extract_barcode(self, page: Page) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑˆÑ‚Ñ€Ğ¸Ñ…ĞºĞ¾Ğ´Ğ°."""
        barcode_selectors = [
            '[data-testid="barcode"]',
            '.barcode',
            '*:has-text("Ğ¨Ñ‚Ñ€Ğ¸Ñ…ĞºĞ¾Ğ´")'
        ]
        
        return await self._extract_text_by_selectors(page, barcode_selectors)
    
    async def _extract_shelf_life(self, page: Page) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ÑÑ€Ğ¾ĞºĞ° Ğ³Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸."""
        shelf_life_selectors = [
            '[data-testid="shelf-life"]',
            '.shelf-life',
            '*:has-text("Ğ¡Ñ€Ğ¾Ğº Ğ³Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚Ğ¸")',
            '*:has-text("Ğ“Ğ¾Ğ´ĞµĞ½ Ğ´Ğ¾")'
        ]
        
        return await self._extract_text_by_selectors(page, shelf_life_selectors)
    
    async def _extract_text_by_selectors(self, page: Page, selectors: List[str]) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ ÑĞ¿Ğ¸ÑĞºÑƒ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ¾Ğ²."""
        for selector in selectors:
            try:
                if ':has-text(' in selector:
                    elements = await page.query_selector_all(selector)
                    for element in elements:
                        parent = await element.query_selector('xpath=..')
                        if parent:
                            text = await parent.inner_text()
                            if text and text.strip():
                                # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ
                                lines = text.split('\n')
                                for line in lines:
                                    if len(line.strip()) > 2 and not any(word in line.lower() for word in ['Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒ', 'Ğ±Ñ€ĞµĞ½Ğ´', 'ÑˆÑ‚Ñ€Ğ¸Ñ…ĞºĞ¾Ğ´', 'ÑÑ€Ğ¾Ğº']):
                                        return line.strip()
                else:
                    element = await page.query_selector(selector)
                    if element:
                        text = await element.inner_text()
                        if text and text.strip():
                            return text.strip()
            except:
                continue
        return None
    
    def _extract_price(self, price_text: str) -> Optional[Decimal]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ†ĞµĞ½Ñ‹ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."""
        if not price_text:
            return None
        
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ²ÑĞµ ĞºÑ€Ğ¾Ğ¼Ğµ Ñ†Ğ¸Ñ„Ñ€, Ñ‚Ğ¾Ñ‡ĞµĞº Ğ¸ Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ñ…
        clean_price = re.sub(r'[^\d.,]', '', price_text)
        
        # Ğ˜Ñ‰ĞµĞ¼ Ñ‡Ğ¸ÑĞ»Ğ¾
        price_match = re.search(r'(\d+(?:[.,]\d+)?)', clean_price)
        if price_match:
            price_str = price_match.group(1).replace(',', '.')
            try:
                return Decimal(price_str)
            except:
                pass
        
        return None
    
    def _extract_weight_from_text(self, text: str) -> Optional[Decimal]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ğ²ĞµÑĞ° Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."""
        if not text:
            return None
        
        # Ğ˜Ñ‰ĞµĞ¼ Ğ²ĞµÑ Ğ² Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ…
        weight_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*Ğ³\b',
            r'(\d+(?:[.,]\d+)?)\s*Ğ³Ñ€\b',
            r'(\d+(?:[.,]\d+)?)\s*gram\b',
            r'(\d+(?:[.,]\d+)?)\s*g\b'
        ]
        
        for pattern in weight_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return Decimal(match.group(1).replace(',', '.'))
                except:
                    continue
        
        return None
    
    def _extract_tags_from_text(self, text: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞ³Ğ¾Ğ² Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°."""
        if not text:
            return []
        
        tags = []
        text_lower = text.lower()
        
        # ĞšĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ´Ğ»Ñ Ñ‚ĞµĞ³Ğ¾Ğ²
        tag_keywords = {
            'Ğ¾ÑÑ‚Ñ€Ğ¾Ğµ': ['Ğ¾ÑÑ‚Ñ€Ñ‹Ğ¹', 'Ğ¾ÑÑ‚Ñ€Ğ°Ñ', 'Ğ¾ÑÑ‚Ñ€Ğ¾Ğµ', 'Ğ¿ĞµÑ€ĞµÑ†', 'Ñ‡Ğ¸Ğ»Ğ¸'],
            'Ğ²ĞµĞ³ĞµÑ‚Ğ°Ñ€Ğ¸Ğ°Ğ½ÑĞºĞ¾Ğµ': ['Ğ²ĞµĞ³ĞµÑ‚Ğ°Ñ€Ğ¸Ğ°Ğ½ÑĞºĞ¸Ğ¹', 'Ğ²ĞµĞ³ĞµÑ‚Ğ°Ñ€Ğ¸Ğ°Ğ½ÑĞºĞ°Ñ', 'Ğ²ĞµĞ³Ğ°Ğ½'],
            'Ğ´Ğ¸ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ': ['Ğ´Ğ¸ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 'Ğ´Ğ¸ĞµÑ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ', 'Ğ¿Ğ¿', 'Ñ„Ğ¸Ñ‚Ğ½ĞµÑ'],
            'Ğ±ĞµĞ· Ğ³Ğ»ÑÑ‚ĞµĞ½Ğ°': ['Ğ±ĞµĞ· Ğ³Ğ»ÑÑ‚ĞµĞ½Ğ°', 'Ğ±ĞµĞ·Ğ³Ğ»ÑÑ‚ĞµĞ½Ğ¾Ğ²Ñ‹Ğ¹'],
            'Ğ±ĞµĞ· Ğ»Ğ°ĞºÑ‚Ğ¾Ğ·Ñ‹': ['Ğ±ĞµĞ· Ğ»Ğ°ĞºÑ‚Ğ¾Ğ·Ñ‹', 'Ğ±ĞµĞ·Ğ»Ğ°ĞºÑ‚Ğ¾Ğ·Ğ½Ñ‹Ğ¹'],
            'Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ': ['Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 'ÑĞºĞ¾', 'Ğ±Ğ¸Ğ¾'],
            'Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½ĞµĞµ': ['Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½Ğ¸Ğ¹', 'Ğ´Ğ¾Ğ¼Ğ°ÑˆĞ½ÑÑ', 'Ñ„ĞµÑ€Ğ¼ĞµÑ€ÑĞºĞ¸Ğ¹']
        }
        
        for tag, keywords in tag_keywords.items():
            if any(keyword in text_lower for keyword in keywords):
                tags.append(tag)
        
        return tags
    
    async def _extract_text_from_element(self, element, selectors: List[str]) -> Optional[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¸Ğ· ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ğ° Ğ¿Ğ¾ ÑĞµĞ»ĞµĞºÑ‚Ğ¾Ñ€Ğ°Ğ¼."""
        for selector in selectors:
            try:
                sub_element = await element.query_selector(selector)
                if sub_element:
                    text = await sub_element.inner_text()
                    if text and text.strip():
                        return text.strip()
            except:
                continue
        return None
    
    def _extract_id_from_url(self, url: str) -> str:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ID Ñ‚Ğ¾Ğ²Ğ°Ñ€Ğ° Ğ¸Ğ· URL."""
        # Ğ˜Ñ‰ĞµĞ¼ ID Ğ² URL Ğ›Ğ°Ğ²ĞºĞ¸
        id_patterns = [
            r'/product/([^/?]+)',
            r'/([^/?]+)/?$',
            r'id=([^&]+)'
        ]
        
        for pattern in id_patterns:
            match = re.search(pattern, url)
            if match:
                return match.group(1)
        
        # Ğ’ ĞºÑ€Ğ°Ğ¹Ğ½ĞµĞ¼ ÑĞ»ÑƒÑ‡Ğ°Ğµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ñ…ĞµÑˆ Ğ¾Ñ‚ URL
        return super()._generate_id_from_url(url)
    
    async def _smart_scroll_to_load_all(self, page) -> None:
        """Ğ£Ğ¼Ğ½Ğ°Ñ Ğ¿Ñ€Ğ¾ĞºÑ€ÑƒÑ‚ĞºĞ° Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº."""
        logger.info("ğŸ”„ Smart scrolling to load all products...")
        
        prev_count = 0
        stable_rounds = 0
        max_rounds = 20
        
        for round_num in range(max_rounds):
            # ĞŸÑ€Ğ¾ĞºÑ€ÑƒÑ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ²Ğ½Ğ¸Ğ·
            await page.mouse.wheel(0, 4000)
            await asyncio.sleep(1)
            
            # Ğ¡Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼ ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞºĞ¸
            try:
                grid = page.locator("[class*='Card'], [class*='Product'], .product-card")
                current_count = await grid.count()
                
                logger.debug(f"   Round {round_num + 1}: {current_count} cards")
                
                if current_count > prev_count:
                    prev_count = current_count
                    stable_rounds = 0
                    logger.debug(f"   âœ… New cards loaded: {current_count}")
                else:
                    stable_rounds += 1
                    logger.debug(f"   â¸ï¸ No new cards: stable round {stable_rounds}")
                
                # Ğ•ÑĞ»Ğ¸ 3 Ñ€Ğ°ÑƒĞ½Ğ´Ğ° Ğ±ĞµĞ· Ğ½Ğ¾Ğ²Ñ‹Ñ… ĞºĞ°Ñ€Ñ‚Ğ¾Ñ‡ĞµĞº - Ğ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ÑÑ
                if stable_rounds >= 3:
                    logger.info(f"âœ… Scrolling completed: {current_count} cards loaded")
                    break
                    
            except Exception as e:
                logger.warning(f"Error during scroll round {round_num + 1}: {e}")
                break
        
        logger.info(f"ğŸ Smart scrolling finished after {round_num + 1} rounds")
