#!/usr/bin/env python3
"""
parsing_worker.py - –í–æ—Ä–∫–µ—Ä –ø–∞—Ä—Å–µ—Ä–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
"""
import asyncio
import json
import logging
import sys
import time
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional

sys.path.append(str(Path(__file__).parent))

from address import VkusvillFastParser, AntiBotClient, get_location_from_address

import redis.asyncio as aioredis
from tenacity import retry, stop_after_attempt, wait_exponential

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ParsingWorker:
    """–í–æ—Ä–∫–µ—Ä –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á –ø–∞—Ä—Å–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ Redis —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º."""

    def __init__(self, redis_url: str):
        self.redis_url = redis_url
        self.redis = None
        self.parsing_queue = "parsing_queue"
        self.results_queue_prefix = "results:"
        self.antibot_client = None
        self.parser = None
        self.base_csv_path = Path("data/moscow_improved_1758362624.csv")
        self.base_df = None
        self.stats = {
            "tasks_processed": 0,
            "tasks_success": 0,
            "tasks_error": 0,
            "total_time": 0,
            "start_time": datetime.now().isoformat()
        }
        self.reconnect_attempts = 0
        self.max_reconnect_attempts = 10

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=4, max=60)
    )
    async def connect_redis(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis —Å retry"""
        try:
            if self.redis:
                try:
                    await self.redis.close()
                except:
                    pass

            self.redis = await aioredis.from_url(
                self.redis_url,
                encoding="utf-8",
                decode_responses=True,
                socket_connect_timeout=10,
                socket_keepalive=True,
                socket_keepalive_options={
                    1: 1,  # TCP_KEEPIDLE
                    2: 10,  # TCP_KEEPINTVL
                    3: 6,  # TCP_KEEPCNT
                }
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
            await self.redis.ping()
            logger.info("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Redis")
            self.reconnect_attempts = 0
            return True

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Redis: {e}")
            self.reconnect_attempts += 1
            raise

    async def ensure_redis_connection(self):
        """–£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ Redis –ø–æ–¥–∫–ª—é—á–µ–Ω"""
        try:
            if self.redis:
                await self.redis.ping()
        except:
            logger.warning("–ü–æ—Ç–µ—Ä—è–Ω–æ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Redis, –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∞–µ–º—Å—è...")
            await self.connect_redis()

    async def connect(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        try:
            # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Redis
            await self.connect_redis()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
            self.antibot_client = AntiBotClient(concurrency=10, timeout=30)
            self.parser = VkusvillFastParser(self.antibot_client)

            # –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
            if self.base_csv_path.exists():
                logger.info(f"üìö –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã: {self.base_csv_path}")
                self.base_df = pd.read_csv(self.base_csv_path)
                logger.info(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.base_df)} –ø—Ä–æ–¥—É–∫—Ç–æ–≤")

                self.parser.heavy_data = {}
                for _, row in self.base_df.iterrows():
                    self.parser.heavy_data[row['id']] = row.to_dict()
            else:
                logger.warning(f"‚ö†Ô∏è –ë–∞–∑–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.base_csv_path}")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            raise

    async def disconnect(self):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –æ—Ç Redis –∏ –æ—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            if self.redis:
                await self.redis.close()
            if self.antibot_client:
                await self.antibot_client.close()
        except:
            pass

    async def send_heartbeat(self):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ heartbeat —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        while True:
            try:
                await self.ensure_redis_connection()

                await self.redis.set(
                    "parser:heartbeat",
                    datetime.now().isoformat(),
                    ex=120
                )

                avg_time = (self.stats["total_time"] / self.stats["tasks_processed"]
                            if self.stats["tasks_processed"] > 0 else 0)

                stats_data = {
                    **self.stats,
                    "avg_time": avg_time,
                    "uptime": (datetime.now() - datetime.fromisoformat(self.stats["start_time"])).total_seconds()
                }

                await self.redis.set(
                    "parser:stats",
                    json.dumps(stats_data),
                    ex=3600
                )

                await asyncio.sleep(30)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ heartbeat: {e}")
                await asyncio.sleep(60)

    async def process_task(self, task: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π –∑–∞–¥–∞—á–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        task_id = task.get("task_id")
        user_id = task.get("user_id")
        mode = task.get("mode", "fast")

        logger.info(f"üì• –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ {task_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

        start_time = time.time()

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ª–∏ –∑–∞–¥–∞—á–∞
            await self.ensure_redis_connection()
            cancelled = await self.redis.sismember("cancelled_tasks", task_id)

            if cancelled:
                logger.info(f"üö´ –ó–∞–¥–∞—á–∞ {task_id} –±—ã–ª–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
                return None

            if mode == "full":
                result_df = await self.run_full_parsing()
            else:
                result_df = await self.run_fast_parsing(task)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result = {
                "status": "success",
                "data": result_df.to_dict('records') if result_df is not None else [],
                "error_message": None
            }

            self.stats["tasks_success"] += 1
            logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ {task_id}: {e}")

            result = {
                "status": "error",
                "data": None,
                "error_message": str(e)
            }

            self.stats["tasks_error"] += 1

        finally:
            self.stats["tasks_processed"] += 1
            self.stats["total_time"] += time.time() - start_time

        return result

    async def run_fast_parsing(self, task: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """–ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ –≥–µ–æ–ª–æ–∫–∞—Ü–∏–∏"""
        coordinates = task.get("coordinates", {})
        lat = coordinates.get("lat", 55.7558)
        lon = coordinates.get("lon", 37.6176)
        address = task.get("address", f"{lat},{lon}")

        logger.info(f"üó∫Ô∏è –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–ª—è: {address}")

        try:
            city = "–ú–æ—Å–∫–≤–∞"
            coords = f"{lat},{lon}"

            products = await self.parser.scrape_fast(
                city=city,
                coords=coords,
                address=address,
                limit=1500
            )

            if products:
                df = pd.DataFrame(products)
                logger.info(f"   –ù–∞–π–¥–µ–Ω–æ {len(df)} –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤")
                return df
            else:
                logger.warning("   –ü—Ä–æ–¥—É–∫—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É")
                if self.base_df is not None:
                    return self.base_df.head(100)
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—ã—Å—Ç—Ä–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            if self.base_df is not None:
                return self.base_df.head(100)
            raise

    async def run_full_parsing(self) -> Optional[pd.DataFrame]:
        """–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ø—Ä–æ–¥—É–∫—Ç–æ–≤"""
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞...")

        try:
            from moscow_improved import VkusvillHeavyParser

            heavy_parser = VkusvillHeavyParser(self.antibot_client)
            products = await heavy_parser.scrape_heavy(limit=1500)

            if products:
                df = pd.DataFrame(products)

                timestamp = int(time.time())
                new_csv_path = Path(f"data/moscow_improved_{timestamp}.csv")
                new_csv_path.parent.mkdir(exist_ok=True)

                df.to_csv(new_csv_path, index=False, encoding='utf-8')
                logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {new_csv_path}")

                self.base_df = df
                self.base_csv_path = new_csv_path

                self.parser.heavy_data = {}
                for _, row in df.iterrows():
                    self.parser.heavy_data[row['id']] = row.to_dict()

                return df
            else:
                logger.warning("–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                return self.base_df

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            raise

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        logger.info("üöÄ –í–æ—Ä–∫–µ—Ä –ø–∞—Ä—Å–µ—Ä–∞ –∑–∞–ø—É—â–µ–Ω")

        # –ó–∞–ø—É—Å–∫–∞–µ–º heartbeat –≤ —Ñ–æ–Ω–µ
        asyncio.create_task(self.send_heartbeat())

        consecutive_errors = 0
        max_consecutive_errors = 10

        while True:
            try:
                await self.ensure_redis_connection()

                # –ë–ª–æ–∫–∏—Ä—É—é—â–µ–µ —á—Ç–µ–Ω–∏–µ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                result = await self.redis.brpop(["parsing_queue"], timeout=5)

                if result:
                    _, task_json = result
                    task = json.loads(task_json)

                    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É
                    result = await self.process_task(task)

                    if result:
                        task_id = task.get("task_id")
                        result_key = f"{self.results_queue_prefix}{task_id}"

                        await self.ensure_redis_connection()
                        await self.redis.set(
                            result_key,
                            json.dumps(result),
                            ex=300
                        )

                        logger.info(f"üì§ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {result_key}")

                    consecutive_errors = 0

            except asyncio.TimeoutError:
                # –¢–∞–π–º–∞—É—Ç - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                consecutive_errors = 0
                continue

            except Exception as e:
                consecutive_errors += 1
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ ({consecutive_errors}/{max_consecutive_errors}): {e}")

                if consecutive_errors >= max_consecutive_errors:
                    logger.critical("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ—à–∏–±–æ–∫, –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–∞–µ–º—Å—è...")
                    await self.disconnect()
                    await asyncio.sleep(30)
                    await self.connect()
                    consecutive_errors = 0
                else:
                    await asyncio.sleep(min(consecutive_errors * 5, 60))


async def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –≤–æ—Ä–∫–µ—Ä–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –ø–µ—Ä–µ–∑–∞–ø—É—Å–∫–æ–º"""
    import os
    from dotenv import load_dotenv

    load_dotenv()

    redis_url = os.getenv("REDIS_PUBLIC_URL", "redis://localhost:6379")

    logger.info("=" * 50)
    logger.info("üöÄ VKUSVILL PARSER WORKER v2.0")
    logger.info(f"üìç Redis: {redis_url}")
    logger.info("=" * 50)

    while True:
        worker = ParsingWorker(redis_url)

        try:
            await worker.connect()
            await worker.run()

        except KeyboardInterrupt:
            logger.info("‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤–æ—Ä–∫–µ—Ä–∞...")
            break

        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            logger.info("–ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ —á–µ—Ä–µ–∑ 60 —Å–µ–∫—É–Ω–¥...")
            await asyncio.sleep(60)

        finally:
            await worker.disconnect()


if __name__ == "__main__":
    asyncio.run(main())