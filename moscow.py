#!/usr/bin/env python3
"""
üèóÔ∏è –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê –î–õ–Ø –ú–û–°–ö–í–´
–ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –∑–∞—Ö–æ–¥–æ–º –≤ –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É —Ç–æ–≤–∞—Ä–∞ –¥–ª—è —Å–±–æ—Ä–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞—è –ë–ñ–£.
"""

import asyncio
import csv
import json
import logging
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urljoin, quote

try:
    from selectolax.parser import HTMLParser
except ImportError:
    HTMLParser = None

# –í—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π AntiBotClient
import httpx


class AntiBotClient:
    """HTTP –∫–ª–∏–µ–Ω—Ç —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π cookies –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã."""

    def __init__(self, concurrency: int = 10, timeout: int = 30):
        self.semaphore = asyncio.Semaphore(concurrency)
        self.timeout = timeout
        self.cookies = {}
        self.client = None  # –•—Ä–∞–Ω–∏–º –∫–ª–∏–µ–Ω—Ç

    async def _ensure_client(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–ª–∏ –ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if self.client is None:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            self.client = httpx.AsyncClient(
                timeout=self.timeout,
                headers=headers,
                cookies=self.cookies,
                follow_redirects=True,
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )
        return self.client

    async def request(self, method: str, url: str, **kwargs):
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å HTTP –∑–∞–ø—Ä–æ—Å —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º cookies."""
        async with self.semaphore:
            client = await self._ensure_client()
            try:
                response = await client.request(method, url, **kwargs)
                # –û–±–Ω–æ–≤–ª—è–µ–º cookies
                self.cookies.update(response.cookies)
                return response
            except httpx.TimeoutException:
                # –ü—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º –∫–ª–∏–µ–Ω—Ç
                await self.close()
                self.client = None
                raise

    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞."""
        if self.client:
            await self.client.aclose()
            self.client = None


class VkusvillHeavyParser:
    """–¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–µ—Ä —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏."""
    
    def __init__(self, antibot_client):
        self.antibot_client = antibot_client
        self.BASE_URL = "https://vkusvill.ru"
        
    async def scrape_heavy(self, limit: int = 1500) -> List[Dict]:
        """–¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å –∑–∞—Ö–æ–¥–æ–º –≤ –∫–∞–∂–¥—É—é –∫–∞—Ä—Ç–æ—á–∫—É."""
        print(f"üèóÔ∏è –ù–∞—á–∏–Ω–∞–µ–º —Ç—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–∞ {limit} —Ç–æ–≤–∞—Ä–æ–≤...")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã
        await self._set_location()
        
        # –°–±–æ—Ä –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        print("üìã –°–æ–±–∏—Ä–∞–µ–º –í–°–ï —Ç–æ–≤–∞—Ä—ã –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã...")
        product_urls = set()
        
        ready_food_categories = [
            "/goods/gotovaya-eda/",
            "/goods/gotovaya-eda/novinki/",
            "/goods/gotovaya-eda/vtorye-blyuda/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-myasom/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-ptitsey/",
            "/goods/gotovaya-eda/vtorye-blyuda/vtorye-blyuda-s-ryboy-i-moreproduktami/",
            "/goods/gotovaya-eda/vtorye-blyuda/garniry-i-vtorye-blyuda-bez-myasa/",
            "/goods/gotovaya-eda/vtorye-blyuda/pasta-pitstsa/",
            "/goods/gotovaya-eda/salaty/",
            "/goods/gotovaya-eda/sendvichi-shaurma-i-burgery/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/malo-kaloriy/",
            "/goods/gotovaya-eda/bolshe-belka-menshe-kaloriy/bolshe-belka/",
            "/goods/gotovaya-eda/okroshki-i-letnie-supy/",
            "/goods/gotovaya-eda/supy/",
            "/goods/gotovaya-eda/zavtraki/",
            "/goods/gotovaya-eda/zavtraki/bliny-i-oladi/",
            "/goods/gotovaya-eda/zavtraki/syrniki-zapekanki-i-rikotniki/",
            "/goods/gotovaya-eda/zavtraki/omlety-i-zavtraki-s-yaytsom/",
            "/goods/gotovaya-eda/zavtraki/kashi/",
            "/goods/gotovaya-eda/zakuski/",
            "/goods/gotovaya-eda/rolly-i-sety/",
            "/goods/gotovaya-eda/onigiri/",
            "/goods/gotovaya-eda/pirogi-pirozhki-i-lepyeshki/",
            "/goods/gotovaya-eda/privezem-goryachim/",
            "/goods/gotovaya-eda/privezem-goryachim/goryachie-napitki/",
            "/goods/gotovaya-eda/tarelka-zdorovogo-pitaniya/",
            "/goods/gotovaya-eda/veganskie-i-postnye-blyuda/",
            "/goods/gotovaya-eda/semeynyy-format/",
            "/goods/gotovaya-eda/kombo-na-kazhdyy-den/",
            "/goods/gotovaya-eda/kukhni-mira/",
            "/goods/gotovaya-eda/kukhni-mira/aziatskaya-kukhnya/",
            "/goods/gotovaya-eda/kukhni-mira/russkaya-kukhnya/",
            "/goods/gotovaya-eda/kukhni-mira/kukhnya-kavkaza/",
            "/goods/gotovaya-eda/kukhni-mira/sredizemnomorskaya-kukhnya/",
            "/goods/gotovaya-eda/bliny-i-oladi/",
            "/goods/gotovaya-eda/khalyal/"
        ]
        
        for category in ready_food_categories:
            try:
                urls = await self._get_category_products(category, 500)
                product_urls.update(urls)
                print(f"   {category}: +{len(urls)} —Ç–æ–≤–∞—Ä–æ–≤")
            except Exception as e:
                print(f"   ‚ùå {category}: {e}")
        
        
        print(f"üì¶ –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(product_urls)} —Å—Å—ã–ª–æ–∫ –Ω–∞ —Ç–æ–≤–∞—Ä—ã")
        
        # –ë–µ—Ä–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤
        product_list = list(product_urls)[:limit * 5]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–ø–∞—Å
        products = []
        
        # –ü–∞—Ä—Å–∏–º —Ç–æ–≤–∞—Ä—ã –±–∞—Ç—á–∞–º–∏
        batch_size = 8
        semaphore = asyncio.Semaphore(6)
        
        for i in range(0, len(product_list), batch_size):
            batch = product_list[i:i + batch_size]
            print(f"üîç –¢–æ–≤–∞—Ä—ã {i+1}-{min(i+batch_size, len(product_list))}/{len(product_list)}")
            
            async def process_product(url):
                async with semaphore:
                    return await self._extract_full_product(url)
            
            tasks = [process_product(url) for url in batch]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, dict) and result:
                    if self._is_ready_food(result):
                        products.append(result)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–∏–º–∏—Ç
                        if len(products) >= limit:
                            print(f"üéØ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç {limit} —Ç–æ–≤–∞—Ä–æ–≤")
                            return products
            
            await asyncio.sleep(1)
        
        print(f"üèÅ –¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        return products
    
    async def _set_location(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ª–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ú–æ—Å–∫–≤—ã."""
        try:
            location_url = f"{self.BASE_URL}/api/location?city=–ú–æ—Å–∫–≤–∞&lat=55.7558&lon=37.6176"
            await self.antibot_client.request(method="GET", url=location_url)
            print("üìç –õ–æ–∫–∞—Ü–∏—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞: –ú–æ—Å–∫–≤–∞ (—Ü–µ–Ω—Ç—Ä)")
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –ª–æ–∫–∞—Ü–∏–∏: {e}")
    
    async def _get_category_products(self, category: str, max_products: int) -> List[str]:
        """–ü–æ–ª—É—á–∏—Ç—å –í–°–ï —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —á–µ—Ä–µ–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏—é."""
        product_urls = set()
        
        # –û–±—ã—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ (page=1,2,3...)
        for page_num in range(1, 100):  # –î–æ 100 —Å—Ç—Ä–∞–Ω–∏—Ü
            try:
                url = f"{self.BASE_URL}{category}?page={page_num}"
                response = await self.antibot_client.request(method="GET", url=url)
                
                if response.status_code != 200:
                    break
                    
                parser = HTMLParser(response.text)
                links = parser.css('a[href*="/goods/"][href$=".html"]')
                
                if not links:
                    break

                page_count = 0
                for link in links:
                    href = link.attributes.get('href')
                    if href and '.html' in href and '/goods/' in href:
                        full_url = urljoin(self.BASE_URL, href)
                        if full_url not in product_urls:
                            product_urls.add(full_url)
                            page_count += 1

                if page_count == 0:  # –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ - –∫–æ–Ω–µ—Ü
                    break
                    
                if len(product_urls) >= max_products:
                    break
                    
                await asyncio.sleep(0.2)
                
            except Exception:
                break
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
        if len(product_urls) < max_products:
            await self._load_more_products(category, product_urls, max_products)
        
        return list(product_urls)
    
    async def _load_more_products(self, category: str, product_urls: set, max_products: int):
        """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø–∞–≥–∏–Ω–∞—Ü–∏—é."""
        # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –±–æ–ª—å—à–µ —Å—Ç—Ä–∞–Ω–∏—Ü —á–µ—Ä–µ–∑ –æ–±—ã—á–Ω—É—é –ø–∞–≥–∏–Ω–∞—Ü–∏—é
        for page_num in range(2, 20):  # –°—Ç—Ä–∞–Ω–∏—Ü—ã 2-19
            try:
                url = f"{self.BASE_URL}{category}?page={page_num}"
                response = await self.antibot_client.request(method="GET", url=url)
                
                if response.status_code != 200:
                    break
                    
                parser = HTMLParser(response.text)
                links = parser.css('a[href*="/goods/"][href$=".html"]')
                
                if not links:
                    break

                page_count = 0
                for link in links:
                    href = link.attributes.get('href')
                    if href and '.html' in href and '/goods/' in href:
                        full_url = urljoin(self.BASE_URL, href)
                        if full_url not in product_urls:
                            product_urls.add(full_url)
                            page_count += 1

                if page_count == 0:  # –ù–µ—Ç –Ω–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ - –∫–æ–Ω–µ—Ü
                    break
                    
                if len(product_urls) >= max_products:
                    break
                    
                await asyncio.sleep(0.2)
                
            except Exception:
                break
    
    async def _search_products(self, search_term: str, max_results: int) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–∏—Å–∫–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É —Å–∞–π—Ç–∞."""
        product_urls = set()
        
        try:
            # URL –ø–æ–∏—Å–∫–∞ –í–∫—É—Å–í–∏–ª–ª
            search_url = f"{self.BASE_URL}/search/"
            
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞
            search_variants = [
                {"q": search_term},
                {"search": search_term},
                {"query": search_term},
            ]
            
            for params in search_variants:
                try:
                    response = await self.antibot_client.request(
                        method="GET", 
                        url=search_url,
                        params=params
                    )
                    
                    if response.status_code == 200:
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        for link in links:
                            href = link.attributes.get('href')
                            if href and '.html' in href and '/goods/' in href:
                                full_url = urljoin(self.BASE_URL, href)
                                product_urls.add(full_url)
                                
                                if len(product_urls) >= max_results:
                                    break
                        
                        if product_urls:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ —Ç–æ–≤–∞—Ä—ã, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º –ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
                            break
                            
                except Exception:
                    continue
                    
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ: –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ POST –∑–∞–ø—Ä–æ—Å
            if not product_urls:
                try:
                    response = await self.antibot_client.request(
                        method="POST",
                        url=search_url,
                        data={"q": search_term},
                        headers={'Content-Type': 'application/x-www-form-urlencoded'}
                    )
                    
                    if response.status_code == 200:
                        parser = HTMLParser(response.text)
                        links = parser.css('a[href*="/goods/"][href$=".html"]')
                        
                        for link in links:
                            href = link.attributes.get('href')
                            if href and '.html' in href and '/goods/' in href:
                                full_url = urljoin(self.BASE_URL, href)
                                product_urls.add(full_url)
                                
                except Exception:
                    pass
                    
        except Exception as e:
            print(f"      –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        
        return list(product_urls)
    
    async def _get_sitemap_products(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ sitemap.xml."""
        product_urls = set()
        
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã sitemap
            sitemap_urls = [
                f"{self.BASE_URL}/sitemap.xml",
                f"{self.BASE_URL}/sitemap_products.xml",
                f"{self.BASE_URL}/robots.txt"  # –ú–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ sitemap
            ]
            
            for sitemap_url in sitemap_urls:
                try:
                    response = await self.antibot_client.request(method="GET", url=sitemap_url)
                    if response.status_code == 200:
                        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã –≤ XML
                        import re
                        urls = re.findall(r'https://vkusvill\.ru/goods/[^<]+\.html', response.text)
                        product_urls.update(urls)
                        
                        if len(product_urls) > 100:  # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –º–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–µ–∫—Ä–∞—â–∞–µ–º
                            break
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"      –û—à–∏–±–∫–∞ sitemap: {e}")
        
        return list(product_urls)
    
    async def _get_products_by_id_range(self, start_id: int, end_id: int, max_products: int) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–æ–≤ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–±–æ—Ä ID."""
        product_urls = set()
        
        try:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ ID –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ
            import random
            test_ids = random.sample(range(start_id, end_id), min(1000, end_id - start_id))
            
            for product_id in test_ids:
                if len(product_urls) >= max_products:
                    break
                    
                try:
                    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã URL
                    test_urls = [
                        f"{self.BASE_URL}/goods/product-{product_id}.html",
                        f"{self.BASE_URL}/goods/{product_id}.html",
                        f"{self.BASE_URL}/product/{product_id}",
                    ]
                    
                    for test_url in test_urls:
                        response = await self.antibot_client.request(method="HEAD", url=test_url)
                        if response.status_code == 200:
                            product_urls.add(test_url)
                            break
                            
                    # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞
                    if len(product_urls) % 50 == 0:
                        await asyncio.sleep(1)
                        
                except Exception:
                    continue
                    
        except Exception as e:
            print(f"      –û—à–∏–±–∫–∞ ID –ø–µ—Ä–µ–±–æ—Ä–∞: {e}")
        
        return list(product_urls)
    
    def _is_ready_food(self, product: Dict) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Ç–æ–≤–∞—Ä –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã."""
        name = product.get('name', '').lower()
        url = product.get('url', '').lower()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        ready_food_keywords = [
            '—Å—É–ø', '—Å–∞–ª–∞—Ç', '–±–æ—Ä—â', '–æ–º–ª–µ—Ç', '–±–ª–∏–Ω—ã', '–∫–∞—à–∞', '–ø–∏—Ü—Ü–∞',
            '–ø–∞—Å—Ç–∞', '–∫–æ—Ç–ª–µ—Ç–∞', '–∑–∞–ø–µ–∫–∞–Ω–∫–∞', '—Å—ã—Ä–Ω–∏–∫–∏', '–ø–ª–æ–≤', '–ª–∞–∑–∞–Ω—å—è',
            '–∫—Ä–µ–º-—Å—É–ø', '—Ö–∞—Ä—á–æ', '—Ü–µ–∑–∞—Ä—å', '–≤–∏–Ω–µ–≥—Ä–µ—Ç', '–º–∏–º–æ–∑–∞',
            '—Ä–∞–≥—É', '–≥—É–ª—è—à', '–∂–∞—Ä–∫–æ–µ', '–±–∏—Ç–æ—á–∫–∏', '—Ç–µ—Ñ—Ç–µ–ª–∏', '—Ñ—Ä–∏–∫–∞–¥–µ–ª—å–∫–∏',
            '–≥–æ–ª—É–±—Ü—ã', '–¥–æ–ª–º–∞', '–º–∞–Ω—Ç—ã', '–ø–µ–ª—å–º–µ–Ω–∏', '–≤–∞—Ä–µ–Ω–∏–∫–∏', '—Ö–∏–Ω–∫–∞–ª–∏',
            '—à–∞—É—Ä–º–∞', '–±—É—Ä–≥–µ—Ä', '—Å—ç–Ω–¥–≤–∏—á', '—Ä—É–ª–µ—Ç', '–ø–∏—Ä–æ–≥', '–∫–∏—à', '—Ç–∞—Ä—Ç',
            '—Ä–∏–∑–æ—Ç—Ç–æ', '–ø–∞—ç–ª—å—è', '–∫–∞—Ä—Ä–∏', '—Ä–∞–º–µ–Ω', '—Ñ–æ', '—Ç–æ–º-—è–º', '–º–∏—Å–æ',
            '–æ–∫—Ä–æ—à–∫–∞', '—Å–æ–ª—è–Ω–∫–∞', '—â–∏', '—É—Ö–∞', '—Ä–∞—Å—Å–æ–ª—å–Ω–∏–∫', '–∫—É–ª–µ—à',
            '–∑–∞–≤—Ç—Ä–∞–∫', '–æ–±–µ–¥', '—É–∂–∏–Ω'
        ]
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –ù–ï –≥–æ—Ç–æ–≤—É—é –µ–¥—É
        exclude_keywords = [
            '–∫—Ä–µ–º –¥–ª—è', '–≥–µ–ª—å –¥–ª—è', '—Å—Ä–µ–¥—Å—Ç–≤–æ –¥–ª—è', '–ø—Ä–æ–∫–ª–∞–¥–∫–∏', '–ø–æ–¥–≥—É–∑–Ω–∏–∫–∏',
            '—à–∞–º–ø—É–Ω—å', '–±–∞–ª—å–∑–∞–º', '–º—ã–ª–æ', '–∑—É–±–Ω–∞—è', '–ø–∞—Å—Ç–∞ –∑—É–±–Ω–∞—è',
            '—á–∏–ø—Å—ã', '—Å—É—Ö–∞—Ä–∏–∫–∏', '–æ—Ä–µ—Ö–∏', '—Å–µ–º–µ—á–∫–∏', '–∫–æ–Ω—Ñ–µ—Ç—ã', '—à–æ–∫–æ–ª–∞–¥',
            '–º–æ–ª–æ–∫–æ', '–∫–µ—Ñ–∏—Ä', '–π–æ–≥—É—Ä—Ç', '—Ç–≤–æ—Ä–æ–≥', '—Å—ã—Ä', '–º–∞—Å–ª–æ', '—è–π—Ü–∞',
            '–º—è—Å–æ', '–∫—É—Ä–∏—Ü–∞', '–≥–æ–≤—è–¥–∏–Ω–∞', '—Å–≤–∏–Ω–∏–Ω–∞', '—Ä—ã–±–∞', '—Ñ–∏–ª–µ',
            '–æ–≤–æ—â–∏', '—Ñ—Ä—É–∫—Ç—ã', '–∫–∞—Ä—Ç–æ—Ñ–µ–ª—å', '–∫–∞–ø—É—Å—Ç–∞', '–º–æ—Ä–∫–æ–≤—å',
            '—Ö–ª–µ–±', '–±–∞—Ç–æ–Ω', '–±—É–ª–∫–∞', '–±–∞–≥–µ—Ç', '–ª–∞–≤–∞—à'
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL –Ω–∞ –≥–æ—Ç–æ–≤—É—é –µ–¥—É
        if 'gotovaya-eda' in url:
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≥–æ—Ç–æ–≤–æ–π –µ–¥—ã
        if any(keyword in name for keyword in ready_food_keywords):
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
            if not any(exclude in name for exclude in exclude_keywords):
                return True
        
        return False
    
    async def _extract_full_product(self, url: str, retry_count: int = 0) -> Optional[Dict]:
        """–ü–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å–æ –≤—Å–µ–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º."""
        max_retries = 2  # –ú–∞–∫—Å–∏–º—É–º 2 –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–∫–∏
        
        try:
            response = await self.antibot_client.request(method="GET", url=url)
            if response.status_code != 200 or not HTMLParser:
                print(f"      ‚ùå HTTP {response.status_code} –¥–ª—è {url}")
                return None
                
            parser = HTMLParser(response.text)
            page_text = response.text
            
            # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            product = {
                'id': self._extract_id(url),
                'name': self._extract_name(parser, page_text),
                'price': self._extract_price(parser, page_text),
                'category': '–ì–æ—Ç–æ–≤–∞—è –µ–¥–∞',
                'url': url,
                'shop': 'vkusvill_heavy',
                'photo': self._extract_photo(parser),
                'composition': self._extract_composition(parser, page_text),
                'tags': '',
                'portion_g': self._extract_portion_weight(parser, page_text)
            }
            
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£
            nutrition = self._extract_bju_comprehensive(parser, page_text)
            product.update(nutrition)
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—è–º
            filled_bju = sum(1 for field in ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g'] if product.get(field))
            has_composition = bool(product.get('composition'))
            
            # –ö—Ä–∞—Ç–∫–∏–µ –ª–æ–≥–∏
            print(f"      üì¶ {product['name'][:40]}... –ë–ñ–£:{filled_bju}/4 –°–æ—Å—Ç–∞–≤:{'‚úì' if has_composition else '‚úó'} –¶–µ–Ω–∞:{product['price'] or '?'}")
            
            # Retry –¥–ª—è —Å–æ—Å—Ç–∞–≤–∞
            if not has_composition and retry_count < max_retries:
                await asyncio.sleep(0.5)
                return await self._extract_full_product(url, retry_count + 1)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –±–∞–∑–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            if not product['name']:
                print(f"      ‚ùå –ù–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –¥–ª—è {url}")
                return None
            
            return product
            
        except Exception as e:
            if retry_count < max_retries:
                await asyncio.sleep(1)
                return await self._extract_full_product(url, retry_count + 1)
            return None
    
    def _extract_id(self, url: str) -> str:
        """ID —Ç–æ–≤–∞—Ä–∞ –∏–∑ URL."""
        match = re.search(r'/goods/([^/]+)\.html', url)
        return match.group(1) if match else str(hash(url))[-8:]
    
    def _extract_name(self, parser, page_text: str) -> str:
        """–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞."""
        selectors = ['h1', '.product-title', '.goods-title']
        for selector in selectors:
            element = parser.css_first(selector)
            if element and element.text(strip=True):
                return element.text(strip=True)[:150]
        return ""
    
    def _extract_price(self, parser, page_text: str) -> str:
        """–¶–µ–Ω–∞ —Ç–æ–≤–∞—Ä–∞ - —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ."""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ü–µ–Ω—ã
        selectors = [
            '.price', '.product-price', '.cost', '.goods-price',
            '[data-testid*="price"]', '[class*="price"]', 
            '.js-product-price', '.current-price'
        ]
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                price_text = element.text(strip=True)
                # –ò—â–µ–º —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Ü–µ–Ω—ã
                numbers = re.findall(r'(\d+(?:[.,]\d+)?)', price_text)
                for num in numbers:
                    try:
                        price_val = float(num.replace(',', '.'))
                        if 10 <= price_val <= 10000:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω
                            return num.replace(',', '.')
                    except ValueError:
                        continue
        
        # –ü–æ–∏—Å–∫ –≤ JSON –¥–∞–Ω–Ω—ã—Ö
        json_patterns = [
            r'"price"\s*:\s*"?(\d+(?:[.,]\d+)?)"?',
            r'"cost"\s*:\s*"?(\d+(?:[.,]\d+)?)"?',
            r'"currentPrice"\s*:\s*"?(\d+(?:[.,]\d+)?)"?'
        ]
        for pattern in json_patterns:
            match = re.search(pattern, page_text, re.I)
            if match:
                try:
                    price_val = float(match.group(1).replace(',', '.'))
                    if 10 <= price_val <= 10000:
                        return match.group(1).replace(',', '.')
                except ValueError:
                    continue
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        text_patterns = [
            r'(\d+(?:[.,]\d+)?)\s*—Ä—É–±',
            r'(\d+(?:[.,]\d+)?)\s*‚ÇΩ',
            r'—Ü–µ–Ω–∞[:\s]*(\d+(?:[.,]\d+)?)',
            r'—Å—Ç–æ–∏–º–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)'
        ]
        for pattern in text_patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                try:
                    price_val = float(match.group(1).replace(',', '.'))
                    if 10 <= price_val <= 10000:
                        return match.group(1).replace(',', '.')
                except ValueError:
                    continue
        
        return ""
    
    def _extract_photo(self, parser) -> str:
        """–§–æ—Ç–æ —Ç–æ–≤–∞—Ä–∞ - —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ."""
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ñ–æ—Ç–æ
        selectors = [
            'img',  # –í—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –Ω–∞—á–∞–ª–∞
            'img[src*="product"]', 
            '.product-image img',
            '[data-testid*="image"] img',
            '.gallery img',
            '.main-image img',
            'img[alt*="product"]',
            'img[src*="goods"]',
            'img[data-src*="product"]',
            'img[data-src*="goods"]'
        ]
        
        for selector in selectors:
            elements = parser.css(selector)
            for element in elements:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º src –∏ data-src
                src = element.attributes.get('src') or element.attributes.get('data-src')
                if src:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ (–Ω–µ –∏–∫–æ–Ω–∫–∞, –Ω–µ –ª–æ–≥–æ—Ç–∏–ø)
                    if any(keyword in src.lower() for keyword in ['product', 'goods', 'catalog', 'upload', 'resize']):
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –Ω–µ –º–∞–ª–µ–Ω—å–∫–∞—è –∏–∫–æ–Ω–∫–∞
                        width = element.attributes.get('width')
                        height = element.attributes.get('height')
                        if width and height:
                            try:
                                w, h = int(width), int(height)
                                if w < 50 or h < 50:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    continue
                            except ValueError:
                                pass
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                        if any(skip in src.lower() for skip in ['icon', 'logo', 'banner', 'button', 'svg']):
                            continue
                        
                        full_url = urljoin(self.BASE_URL, src)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ URL –≤–∞–ª–∏–¥–Ω—ã–π
                        if full_url.startswith('http'):
                            return full_url
                    
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, –±–µ—Ä–µ–º –ª—é–±–æ–µ –±–æ–ª—å—à–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                    elif src.startswith('/') and any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                        width = element.attributes.get('width')
                        height = element.attributes.get('height')
                        if width and height:
                            try:
                                w, h = int(width), int(height)
                                if w >= 100 and h >= 100:  # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –±–æ–ª—å—à–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                    full_url = urljoin(self.BASE_URL, src)
                                    return full_url
                            except ValueError:
                                pass
        
        return ""
    
    def _extract_composition(self, parser, page_text: str) -> str:
        """–°–æ—Å—Ç–∞–≤ —Ç–æ–≤–∞—Ä–∞ - –ø—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ."""
        # –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤–æ–º—É —Å–ª–æ–≤—É "–°–æ—Å—Ç–∞–≤"
        elements = parser.css('div, p, span, td, li')
        for element in elements:
            text = element.text().strip()
            text_lower = text.lower()
            
            if '—Å–æ—Å—Ç–∞–≤' in text_lower and len(text) > 10:
                if not any(word in text_lower for word in ['–º–µ–Ω—é', '–∫–∞—Ç–∞–ª–æ–≥', '–∫–æ—Ä–∑–∏–Ω–∞', '–≤–∫—É—Å–≤–∏–ª–ª', '–¥–æ—Å—Ç–∞–≤–∫–∏', '–≤—ã–±–µ—Ä–∏—Ç–µ']):
                    if text_lower.startswith('—Å–æ—Å—Ç–∞–≤'):
                        return text[:800]
                    elif len(text) < 800:
                        return text[:500]
        
        return ""
    
    def _extract_portion_weight(self, parser, page_text: str) -> str:
        """–í–µ—Å –ø–æ—Ä—Ü–∏–∏."""
        patterns = [r'(\d+(?:[.,]\d+)?)\s*(?:–≥|–≥—Ä|–≥—Ä–∞–º)']
        for pattern in patterns:
            matches = re.finditer(pattern, page_text, re.I)
            for match in matches:
                weight = float(match.group(1).replace(',', '.'))
                if 10 <= weight <= 2000:
                    return f"{weight}–≥"
        return ""
    
    def _extract_bju_comprehensive(self, parser, page_text: str) -> Dict[str, str]:
        """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –ª–æ–≥–∞–º–∏."""
        nutrition = {'kcal_100g': '', 'protein_100g': '', 'fat_100g': '', 'carb_100g': ''}
        # –ú–µ—Ç–æ–¥ 1: –ü–æ–∏—Å–∫ –≤ JSON-LD
        self._extract_nutrition_from_jsonld(page_text, nutrition)
        
        # –ú–µ—Ç–æ–¥ 2: –ü–æ–∏—Å–∫ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö
        self._extract_nutrition_from_tables(parser, nutrition)
        
        # –ú–µ—Ç–æ–¥ 3: –ü–æ–∏—Å–∫ –≤ —ç–ª–µ–º–µ–Ω—Ç–∞—Ö —Å—Ç—Ä–∞–Ω–∏—Ü—ã (—É–ª—É—á—à–µ–Ω–Ω—ã–π)
        bju_elements_found = 0
        elements = parser.css('div, span, p, td, th, li')
        for element in elements:
            text = element.text().lower()
            original_text = element.text()
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã —Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —á–∏—Å–µ–ª (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –º—É—Å–æ—Ä)
            numbers_count = len(re.findall(r'\d+', original_text))
            if numbers_count > 10:
                continue
                
            if any(word in text for word in ['–∫–∫–∞–ª', '–±–µ–ª–∫–∏', '–∂–∏—Ä—ã', '—É–≥–ª–µ–≤–æ–¥—ã', '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è', '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å', '–ø–∏—â–µ–≤–∞—è', '—Ü–µ–Ω–Ω–æ—Å—Ç—å', '—Å–æ—Å—Ç–∞–≤']):
                bju_elements_found += 1
                
                # –ò—â–µ–º —á–∏—Å–ª–æ –†–Ø–î–û–ú —Å –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–æ–º
                if ('–∫–∫–∞–ª' in text or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in text or '—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è' in text) and not nutrition['kcal_100g']:
                    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–∞–ª–æ—Ä–∏–π (–≤–∫–ª—é—á–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É –í–∫—É—Å–í–∏–ª–ª)
                    kcal_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª',  # "189.6 –ö–∫–∞–ª"
                        r'–∫–∫–∞–ª[:\s]*(\d+(?:[.,]\d+)?)',
                        r'–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                        r'—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è\s+—Ü–µ–Ω–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                        r'(\d+(?:[.,]\d+)?)\s+–∫–∫–∞–ª'  # "189.6 –ö–∫–∞–ª" —Å –ø—Ä–æ–±–µ–ª–æ–º
                    ]
                    for pattern in kcal_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 10 <= val <= 900:
                                    nutrition['kcal_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '–±–µ–ª–∫' in text and not nutrition['protein_100g']:
                    protein_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+–±–µ–ª–∫–∏,\s*–≥',  # "11 –ë–µ–ª–∫–∏, –≥"
                        r'–±–µ–ª–∫[–∏–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'–±–µ–ª–æ–∫[:\s]*(\d+(?:[.,]\d+)?)',
                        r'(\d+(?:[.,]\d+)?)\s*–≥\s*–±–µ–ª–∫'
                    ]
                    for pattern in protein_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['protein_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '–∂–∏—Ä' in text and not nutrition['fat_100g']:
                    fat_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+–∂–∏—Ä—ã,\s*–≥',  # "7.6 –ñ–∏—Ä—ã, –≥"
                        r'–∂–∏—Ä[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'–∂–∏—Ä[:\s]*(\d+(?:[.,]\d+)?)',
                        r'(\d+(?:[.,]\d+)?)\s*–≥\s*–∂–∏—Ä'
                    ]
                    for pattern in fat_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['fat_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
                
                if '—É–≥–ª–µ–≤–æ–¥' in text and not nutrition['carb_100g']:
                    carb_patterns = [
                        r'(\d+(?:[.,]\d+)?)\s+—É–≥–ª–µ–≤–æ–¥—ã,\s*–≥',  # "19.3 –£–≥–ª–µ–≤–æ–¥—ã, –≥"
                        r'—É–≥–ª–µ–≤–æ–¥[—ã–∞][:\s]*(\d+(?:[.,]\d+)?)',
                        r'—É–≥–ª–µ–≤–æ–¥[:\s]*(\d+(?:[.,]\d+)?)',
                        r'(\d+(?:[.,]\d+)?)\s*–≥\s*—É–≥–ª–µ–≤–æ–¥'
                    ]
                    for pattern in carb_patterns:
                        match = re.search(pattern, text)
                        if match:
                            try:
                                val = float(match.group(1).replace(',', '.'))
                                if 0 <= val <= 100:
                                    nutrition['carb_100g'] = match.group(1).replace(',', '.')
                                    break
                            except ValueError:
                                continue
        
        # –ú–µ—Ç–æ–¥ 4: –ü–æ–∏—Å–∫ –ø–æ —Ä–µ–≥—É–ª—è—Ä–Ω—ã–º –≤—ã—Ä–∞–∂–µ–Ω–∏—è–º –≤ —Ç–µ–∫—Å—Ç–µ
        patterns = {
            'kcal_100g': [
                r'(\d+(?:[.,]\d+)?)\s+–∫–∫–∞–ª',  # "189.6 –ö–∫–∞–ª" - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω –í–∫—É—Å–í–∏–ª–ª
                r'(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª',
                r'–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                r'—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∞—è\s+—Ü–µ–Ω–Ω–æ—Å—Ç—å[:\s]*(\d+(?:[.,]\d+)?)',
                r'—ç–Ω–µ—Ä–≥–∏—è[:\s]*(\d+(?:[.,]\d+)?)\s*–∫–∫–∞–ª'
            ],
            'protein_100g': [
                r'(\d+(?:[.,]\d+)?)\s+–±–µ–ª–∫–∏,\s*–≥',  # "11 –ë–µ–ª–∫–∏, –≥" - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω –í–∫—É—Å–í–∏–ª–ª
                r'–±–µ–ª–∫–∏[:\s]*(\d+(?:[.,]\d+)?)',
                r'–±–µ–ª–æ–∫[:\s]*(\d+(?:[.,]\d+)?)',
                r'–ø—Ä–æ—Ç–µ–∏–Ω[:\s]*(\d+(?:[.,]\d+)?)'
            ],
            'fat_100g': [
                r'(\d+(?:[.,]\d+)?)\s+–∂–∏—Ä—ã,\s*–≥',  # "7.6 –ñ–∏—Ä—ã, –≥" - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω –í–∫—É—Å–í–∏–ª–ª
                r'–∂–∏—Ä—ã[:\s]*(\d+(?:[.,]\d+)?)',
                r'–∂–∏—Ä[:\s]*(\d+(?:[.,]\d+)?)'
            ],
            'carb_100g': [
                r'(\d+(?:[.,]\d+)?)\s+—É–≥–ª–µ–≤–æ–¥—ã,\s*–≥',  # "19.3 –£–≥–ª–µ–≤–æ–¥—ã, –≥" - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω –í–∫—É—Å–í–∏–ª–ª
                r'—É–≥–ª–µ–≤–æ–¥—ã[:\s]*(\d+(?:[.,]\d+)?)',
                r'—É–≥–ª–µ–≤–æ–¥[:\s]*(\d+(?:[.,]\d+)?)'
            ]
        }
        
        for field, field_patterns in patterns.items():
            if nutrition[field]:
                continue
            for pattern in field_patterns:
                matches = list(re.finditer(pattern, page_text, re.I))
                for match in matches:
                    try:
                        value = float(match.group(1).replace(',', '.'))
                        if field == 'kcal_100g' and 10 <= value <= 900:
                            nutrition[field] = str(value)
                            break
                        elif field != 'kcal_100g' and 0 <= value <= 100:
                            nutrition[field] = str(value)
                            break
                    except ValueError:
                        continue
                if nutrition[field]:
                    break
        
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–ñ–£
        filled_total = sum(1 for v in nutrition.values() if v)
        
        return nutrition
    
    def _extract_nutrition_from_jsonld(self, page_text: str, nutrition: Dict[str, str]):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£ –∏–∑ JSON-LD."""
        try:
            blocks = re.findall(r'<script[^>]+type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', page_text, re.S|re.I)
            for raw in blocks:
                try:
                    data = json.loads(raw)
                    print(f"         JSON-LD –±–ª–æ–∫ –Ω–∞–π–¥–µ–Ω")
                    
                    def visit(obj):
                        if isinstance(obj, dict):
                            if obj.get('@type') in ('NutritionInformation', 'Nutrition'):
                                print(f"         –ù–∞–π–¥–µ–Ω NutritionInformation –±–ª–æ–∫!")
                                kcal = obj.get('calories') or obj.get('energy')
                                protein = obj.get('proteinContent')
                                fat = obj.get('fatContent')
                                carb = obj.get('carbohydrateContent')
                                
                                if kcal: nutrition['kcal_100g'] = str(kcal)
                                if protein: nutrition['protein_100g'] = str(protein)
                                if fat: nutrition['fat_100g'] = str(fat)
                                if carb: nutrition['carb_100g'] = str(carb)
                            
                            for v in obj.values():
                                visit(v)
                        elif isinstance(obj, list):
                            for v in obj:
                                visit(v)
                    
                    visit(data)
                except:
                    continue
        except:
            pass
    
    def _extract_nutrition_from_tables(self, parser, nutrition: Dict[str, str]):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ë–ñ–£ –∏–∑ —Ç–∞–±–ª–∏—Ü."""
        try:
            tables = parser.css('table')
            print(f"         –ù–∞–π–¥–µ–Ω–æ —Ç–∞–±–ª–∏—Ü: {len(tables)}")
            for i, table in enumerate(tables):
                print(f"         –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É {i+1}")
                rows = table.css('tr')
                for row in rows:
                    cells = row.css('td, th')
                    if len(cells) >= 2:
                        header = cells[0].text().lower()
                        value_text = cells[1].text()
                        
                        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–∞
                        num_match = re.search(r'(\d+(?:[.,]\d+)?)', value_text)
                        if num_match:
                            value = num_match.group(1).replace(',', '.')
                            print(f"         –°—Ç—Ä–æ–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: '{header}' = '{value}'")
                            
                            if ('–∫–∫–∞–ª' in header or '–∫–∞–ª–æ—Ä–∏–π–Ω–æ—Å—Ç—å' in header) and not nutrition['kcal_100g']:
                                nutrition['kcal_100g'] = value
                                print(f"         ‚úÖ –ò–∑ —Ç–∞–±–ª–∏—Ü—ã –∫–∫–∞–ª: {value}")
                            elif '–±–µ–ª–∫' in header and not nutrition['protein_100g']:
                                nutrition['protein_100g'] = value
                                print(f"         ‚úÖ –ò–∑ —Ç–∞–±–ª–∏—Ü—ã –±–µ–ª–∫–∏: {value}")
                            elif '–∂–∏—Ä' in header and not nutrition['fat_100g']:
                                nutrition['fat_100g'] = value
                                print(f"         ‚úÖ –ò–∑ —Ç–∞–±–ª–∏—Ü—ã –∂–∏—Ä—ã: {value}")
                            elif '—É–≥–ª–µ–≤–æ–¥' in header and not nutrition['carb_100g']:
                                nutrition['carb_100g'] = value
                                print(f"         ‚úÖ –ò–∑ —Ç–∞–±–ª–∏—Ü—ã —É–≥–ª–µ–≤–æ–¥—ã: {value}")
        except:
            pass


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç—è–∂–µ–ª–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞."""
    limit = int(sys.argv[1]) if len(sys.argv) > 1 else 1500
    
    print("üèóÔ∏è –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ï–† –í–ö–£–°–í–ò–õ–õ–ê - –ú–û–°–ö–í–ê")
    print("=" * 50)
    print(f"üéØ –¶–µ–ª—å: {limit} —Ç–æ–≤–∞—Ä–æ–≤ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
    print("üìç –ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ: –ú–æ—Å–∫–≤–∞")
    print("‚ö° –†–µ–∂–∏–º: –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –∫–∞—Ä—Ç–æ—á–∫–∏")
    print()
    
    logging.basicConfig(level=logging.WARNING, format='%(asctime)s - %(levelname)s - %(message)s')
    
    antibot_client = AntiBotClient(concurrency=8, timeout=60)
    
    try:
        parser = VkusvillHeavyParser(antibot_client)
        
        start_time = time.time()
        products = await parser.scrape_heavy(limit)
        end_time = time.time()
        
        if not products:
            print("‚ùå –¢—è–∂–µ–ª—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            return
        
        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–ñ–£ –∏ —Å–æ—Å—Ç–∞–≤–∞
        bju_stats = {'full_bju': 0, 'good_bju': 0, 'some_bju': 0, 'no_bju': 0}
        composition_stats = {'has_composition': 0, 'no_composition': 0}
        quality_stats = {'excellent': 0, 'good': 0, 'poor': 0}
        
        for product in products:
            bju_fields = ['kcal_100g', 'protein_100g', 'fat_100g', 'carb_100g']
            filled = sum(1 for field in bju_fields if product.get(field))
            has_composition = bool(product.get('composition'))
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ë–ñ–£
            if filled == 4:
                bju_stats['full_bju'] += 1
            elif filled == 3:
                bju_stats['good_bju'] += 1
            elif filled >= 1:
                bju_stats['some_bju'] += 1
            else:
                bju_stats['no_bju'] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ—Å—Ç–∞–≤–∞
            if has_composition:
                composition_stats['has_composition'] += 1
            else:
                composition_stats['no_composition'] += 1
            
            # –û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            if filled >= 3 and has_composition:
                quality_stats['excellent'] += 1
            elif filled >= 2 or has_composition:
                quality_stats['good'] += 1
            else:
                quality_stats['poor'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = int(time.time())
        csv_file = f"data/moscow_heavy_{timestamp}.csv"
        jsonl_file = f"data/moscow_heavy_{timestamp}.jsonl"
        
        Path("data").mkdir(exist_ok=True)
        
        if products:
            fieldnames = list(products[0].keys())
            with open(csv_file, 'w', newline='', encoding='utf-8') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(products)
        
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for product in products:
                f.write(json.dumps(product, ensure_ascii=False) + '\n')
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∏—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        duration = end_time - start_time
        print()
        print("üèÅ –¢–Ø–ñ–ï–õ–´–ô –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù")
        print("=" * 60)
        print(f"üìä –û–ë–©–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
        print(f"   ‚Ä¢ –í—Å–µ–≥–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"   ‚Ä¢ –°–∫–æ—Ä–æ—Å—Ç—å: {len(products)/(duration/60):.1f} —Ç–æ–≤–∞—Ä–æ–≤/–º–∏–Ω")
        print()
        print(f"üçΩÔ∏è –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ñ–£:")
        print(f"   ‚Ä¢ –ü–æ–ª–Ω–æ–µ –ë–ñ–£ (4/4): {bju_stats['full_bju']} ({bju_stats['full_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ –ë–ñ–£ (3/4): {bju_stats['good_bju']} ({bju_stats['good_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ß–∞—Å—Ç–∏—á–Ω–æ–µ –ë–ñ–£ (1-2/4): {bju_stats['some_bju']} ({bju_stats['some_bju']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ë–µ–∑ –ë–ñ–£ (0/4): {bju_stats['no_bju']} ({bju_stats['no_bju']/len(products)*100:.1f}%)")
        print()
        print(f"üìù –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–û–°–¢–ê–í–ê:")
        print(f"   ‚Ä¢ –ï—Å—Ç—å —Å–æ—Å—Ç–∞–≤: {composition_stats['has_composition']} ({composition_stats['has_composition']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ù–µ—Ç —Å–æ—Å—Ç–∞–≤–∞: {composition_stats['no_composition']} ({composition_stats['no_composition']/len(products)*100:.1f}%)")
        print()
        print(f"‚≠ê –û–ë–©–ï–ï –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•:")
        print(f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω–æ–µ (–ë–ñ–£ 3+ + —Å–æ—Å—Ç–∞–≤): {quality_stats['excellent']} ({quality_stats['excellent']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –•–æ—Ä–æ—à–µ–µ (–ë–ñ–£ 2+ –ò–õ–ò —Å–æ—Å—Ç–∞–≤): {quality_stats['good']} ({quality_stats['good']/len(products)*100:.1f}%)")
        print(f"   ‚Ä¢ –ü–ª–æ—Ö–æ–µ (–ë–ñ–£ <2 –ò –Ω–µ—Ç —Å–æ—Å—Ç–∞–≤–∞): {quality_stats['poor']} ({quality_stats['poor']/len(products)*100:.1f}%)")
        print()
        print(f"‚è±Ô∏è  –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration/60:.1f} –º–∏–Ω—É—Ç")
        print(f"üíæ –§–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"   ‚Ä¢ CSV: {csv_file}")
        print(f"   ‚Ä¢ JSONL: {jsonl_file}")
            
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç—è–∂–µ–ª–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
    finally:
        await antibot_client.close()


if __name__ == "__main__":
    asyncio.run(main())
